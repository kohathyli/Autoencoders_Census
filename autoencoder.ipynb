{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipeirotis/autoencoders_census/blob/main/autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "FcGe1770A3ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgC-DNoNxTC1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Dense, BatchNormalization, Concatenate, Dropout\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras_tuner.tuners import RandomSearch, BayesianOptimization\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class CustomCategoricalCrossentropy(tf.keras.losses.Loss):\n",
        "    def __init__(self, attribute_cardinalities, name=\"custom_categorical_crossentropy\"):\n",
        "        super(CustomCategoricalCrossentropy, self).__init__(name=name)\n",
        "        self.attribute_cardinalities = attribute_cardinalities\n",
        "        log_cardinalities = [np.log(cardinality) for cardinality in self.attribute_cardinalities]\n",
        "        log_cardinalities_tensor = tf.constant(log_cardinalities, dtype=tf.float32)\n",
        "        self.log_cardinalities_expanded = tf.expand_dims(log_cardinalities_tensor, axis=-1)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Your custom loss logic here\n",
        "        y_true_splits = tf.split(y_true, self.attribute_cardinalities, axis=1)\n",
        "        y_pred_splits = tf.split(y_pred, self.attribute_cardinalities, axis=1)\n",
        "\n",
        "        max_size = max(self.attribute_cardinalities)\n",
        "\n",
        "        y_true_splits = [tf.pad(split, [[0, 0], [0, max_size - tf.shape(split)[1]]]) for split in y_true_splits]\n",
        "        y_pred_splits = [tf.pad(split, [[0, 0], [0, max_size - tf.shape(split)[1]]]) for split in y_pred_splits]\n",
        "\n",
        "        xent_losses = tf.keras.losses.categorical_crossentropy(y_true_splits, y_pred_splits)\n",
        "\n",
        "        normalized_xent_losses = xent_losses / self.log_cardinalities_expanded\n",
        "\n",
        "        return tf.reduce_mean(normalized_xent_losses, axis=0)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'attribute_cardinalities': self.attribute_cardinalities}\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class AutoencoderModel:\n",
        "    def __init__(self, attribute_cardinalities):\n",
        "        self.INPUT_SHAPE = None\n",
        "        # self.D = 2\n",
        "        self.TEST_SIZE = 0.2\n",
        "        self.MAX_TRIALS = 20\n",
        "        self.EXECUTIONS_PER_TRIAL = 1\n",
        "        self.BATCH_SIZE = 32\n",
        "        self.attribute_cardinalities = attribute_cardinalities\n",
        "\n",
        "        log_cardinalities = [np.log(cardinality) for cardinality in self.attribute_cardinalities]\n",
        "        log_cardinalities_tensor = tf.constant(log_cardinalities, dtype=tf.float32)\n",
        "        self.log_cardinalities_expanded = tf.expand_dims(log_cardinalities_tensor, axis=-1)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'attribute_cardinalities': self.attribute_cardinalities}\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "    def split_train_test(self, df):\n",
        "        # df = df.fillna(0.0)\n",
        "        X_train, X_test = train_test_split(df.copy(), test_size=self.TEST_SIZE)\n",
        "        self.INPUT_SHAPE = X_train.shape[1:]\n",
        "        return X_train.dropna(), X_test.dropna()\n",
        "\n",
        "    @staticmethod\n",
        "    def masked_mse(y_true, y_pred):\n",
        "        mask = tf.math.is_finite(y_true)\n",
        "        y_t = tf.where(tf.math.is_finite(y_true), y_true, 0.0)\n",
        "        y_p = tf.where(tf.math.is_finite(y_pred), y_pred, 0.0)\n",
        "        mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
        "        return tf.reduce_mean(mse(y_t*tf.cast(mask, y_t.dtype), y_p*tf.cast(mask, y_p.dtype)))\n",
        "\n",
        "    def build_encoder(self, hp):\n",
        "        inputs = Input(shape=self.INPUT_SHAPE)\n",
        "        x = Dense(units=hp.Int('encoder_units_1', min_value=160, max_value=160, step=16),\n",
        "                  activation='relu',\n",
        "                  kernel_regularizer=l2(hp.Choice('encoder_l2_1', [0.0, 0.001])))(inputs)\n",
        "        x = Dropout(hp.Float('encoder_dropout_1', 0.1, 0.1, step=0.1))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(units=hp.Int('encoder_units_2', min_value=16, max_value=16, step=16),\n",
        "                  activation='relu',\n",
        "                  kernel_regularizer=l2(hp.Choice('encoder_l2_2', [0.0, 0.001])))(x)\n",
        "        x = Dropout(hp.Float('encoder_dropout_2', 0, 0.5, step=0.1))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        latent_space = Dense(units=hp.Int('latent_space_dim', min_value=2, max_value=50, step=1),\n",
        "                             activation='relu')(x)\n",
        "        return Model(inputs, latent_space)\n",
        "\n",
        "    def build_decoder(self, hp):\n",
        "        decoder_inputs = Input(shape=(hp.Int('latent_space_dim', min_value=2, max_value=50, step=1),))\n",
        "        x = Dense(units=hp.Int('decoder_units_1', min_value=16, max_value=256, step=16),\n",
        "                  activation='relu',\n",
        "                  kernel_regularizer=l2(hp.Choice('decoder_l2_1', [0.0, 0.001, 0.01])))(decoder_inputs)\n",
        "        x = Dropout(hp.Float('decoder_dropout_1', 0.0, 0.0, step=0.1))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(units=hp.Int('decoder_units_2', min_value=160, max_value=256, step=16),\n",
        "                  activation='relu',\n",
        "                  kernel_regularizer=l2(hp.Choice('decoder_l2_2', [0.0, 0.001])))(x)\n",
        "        x = Dropout(hp.Float('decoder_dropout_2', 0, 0.5, step=0.1))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        decoded_attrs = []\n",
        "        for categories in self.attribute_cardinalities:\n",
        "          decoder_softmax = Dense(categories, activation='softmax')(x)\n",
        "          decoded_attrs.append(decoder_softmax)\n",
        "\n",
        "        outputs = Concatenate()(decoded_attrs)\n",
        "\n",
        "        return Model(decoder_inputs, outputs)\n",
        "\n",
        "    '''\n",
        "    def custom_categorical_crossentropy(self, y_true, y_pred):\n",
        "        xent_loss = 0\n",
        "        start_idx = 0\n",
        "\n",
        "        for categories in self.attribute_cardinalities:\n",
        "            x_attr = y_true[:, start_idx:start_idx + categories]\n",
        "            y_attr = y_pred[:, start_idx:start_idx + categories]\n",
        "\n",
        "            x_attr = K.cast(x_attr, 'float32')\n",
        "            y_attr = K.cast(y_attr, 'float32')\n",
        "\n",
        "            xent_loss += K.mean(K.categorical_crossentropy(x_attr, y_attr)) / np.log(categories)\n",
        "\n",
        "            start_idx += categories\n",
        "        return xent_loss / len(self.attribute_cardinalities)\n",
        "    '''\n",
        "    '''\n",
        "    def custom_categorical_crossentropy(self, y_true, y_pred):\n",
        "        # We create a separate vector for each attribute\n",
        "        y_true_splits = tf.split(y_true, self.attribute_cardinalities, axis=1)\n",
        "        y_pred_splits = tf.split(y_pred, self.attribute_cardinalities, axis=1)\n",
        "\n",
        "        # Compute the maximum size among the splits\n",
        "        max_size = max(self.attribute_cardinalities)\n",
        "\n",
        "        # Pad each split to have the same size as max_size, this will be useful for calculating\n",
        "        # cross entropy for each vector. If the shape of the vectors is not consistent, the K.categorical_crossentropy\n",
        "        # does not work.\n",
        "        y_true_splits = [tf.pad(split, [[0, 0], [0, max_size - tf.shape(split)[1]]]) for split in y_true_splits]\n",
        "        y_pred_splits = [tf.pad(split, [[0, 0], [0, max_size - tf.shape(split)[1]]]) for split in y_pred_splits]\n",
        "\n",
        "        # Compute the categorical cross-entropy for each attribute\n",
        "        xent_losses = K.categorical_crossentropy(y_true_splits, y_pred_splits)\n",
        "\n",
        "        # Normalize by log of cardinality\n",
        "        normalized_xent_losses = xent_losses / self.log_cardinalities_expanded\n",
        "\n",
        "        return K.mean(normalized_xent_losses,axis=0)\n",
        "    '''\n",
        "\n",
        "    def build_autoencoder(self, hp):\n",
        "        learning_rate = hp.Choice('learning_rate', values=[1e-3])\n",
        "\n",
        "        autoencoder_input = Input(shape=self.INPUT_SHAPE)\n",
        "        encoder_output = self.build_encoder(hp)(autoencoder_input)\n",
        "        decoder_output = self.build_decoder(hp)(encoder_output)\n",
        "        autoencoder = Model(autoencoder_input, decoder_output)\n",
        "        autoencoder.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "            loss=CustomCategoricalCrossentropy(attribute_cardinalities=self.attribute_cardinalities)\n",
        "        )\n",
        "\n",
        "        return autoencoder\n",
        "\n",
        "    def define_tuner(self, seed_hps=None):\n",
        "        tuner = BayesianOptimization(\n",
        "            self.build_autoencoder,\n",
        "            objective='val_loss',\n",
        "            max_trials=self.MAX_TRIALS,\n",
        "            executions_per_trial=self.EXECUTIONS_PER_TRIAL,\n",
        "            hyperparameters=seed_hps\n",
        "            )\n",
        "        return tuner\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class TestAutoencoderModel(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        # Create a DataFrame with synthetic data\n",
        "        self.df = pd.DataFrame({\n",
        "            'col1': ['A', 'B', 'A', 'C', 'B'],\n",
        "            'col2': [1, 2, 1, 2, 2],\n",
        "            'col3': ['X', 'Y', 'X', 'Y', 'Z']\n",
        "        })\n",
        "        self.attribute_cardinalities = [3, 2, 3]  # 3 unique categories in each attribute\n",
        "        self.autoencoder = AutoencoderModel(self.attribute_cardinalities)\n",
        "\n",
        "    def test_split_train_test(self):\n",
        "        X_train, X_test = self.autoencoder.split_train_test(self.df)\n",
        "        self.assertEqual(len(X_train) + len(X_test), len(self.df))\n",
        "\n",
        "    def test_masked_mse(self):\n",
        "        y_true = np.array([[1.0, np.nan], [2.0, 2.0]])\n",
        "        y_pred = np.array([[1.0, 1.0], [3.0, 2.0]])\n",
        "        result = self.autoencoder.masked_mse(y_true, y_pred)\n",
        "        self.assertIsNotNone(result)\n",
        "\n",
        "    def test_custom_categorical_crossentropy(self):\n",
        "        y_true = np.array([[1, 0, 0, 1, 0, 1, 0, 0],\n",
        "                           [0, 1, 0, 0, 1, 0, 0, 1]])\n",
        "        y_pred = np.array([[0.8, 0.1, 0.1, 0.7, 0.3, 0.9, 0.05, 0.05],\n",
        "                           [0.1, 0.8, 0.1, 0.3, 0.7, 0.05, 0.05, 0.9]])\n",
        "        result = CustomCategoricalCrossentropy(attribute_cardinalities=self.attribute_cardinalities)(y_true, y_pred)\n",
        "        # p=[1,0,0],q=[0.8,0.1,0.1]: 0.2231 / np.log(3)\n",
        "        # p=[1,0],q=[0.7,0.3]: 0.3567 / np.log(2)\n",
        "        # p=[1,0,0],q=[0.9,0.05,0.05]: 0.1054 / np.log(3)\n",
        "        # p=[0,1,0],q=[0.1,0.8,0.1]: 0.2231 / np.log(3)\n",
        "        # p=[0,1,0],q=[0.3,0.7]: 0.3567 / np.log(2)\n",
        "        # p=[0,0,1],q=[0.05,0.05,0.9]: 0.1054 / np.log(3)\n",
        "        print(result)\n",
        "        self.assertIsNotNone(result)\n",
        "        # We divide by 2 because we have two examples\n",
        "        # We divide by 3 because we have 3 attributes per example\n",
        "        # We divide by np.log(3) for normalization (all attributes have cardinality 3)\n",
        "        self.assertAlmostEqual(result.mean(), (0.2231/ np.log(3) + 0.3567/ np.log(2) + 0.1054/ np.log(3))/3, places=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "OcWSTfn9vVPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "def run_tests(test_class):\n",
        "    suite = unittest.TestLoader().loadTestsFromTestCase(test_class)\n",
        "    runner = unittest.TextTestRunner()\n",
        "    runner.run(suite)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  run_tests(TestAutoencoderModel)"
      ],
      "metadata": {
        "id": "sURHG2b_wOjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Dense, BatchNormalization, Concatenate, Dropout\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras_tuner.tuners import RandomSearch, BayesianOptimization\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Lambda\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class CustomCategoricalCrossentropy(tf.keras.losses.Loss):\n",
        "    def __init__(self, attribute_cardinalities, name=\"custom_categorical_crossentropy\"):\n",
        "        super(CustomCategoricalCrossentropy, self).__init__(name=name)\n",
        "        self.attribute_cardinalities = attribute_cardinalities\n",
        "        log_cardinalities = [np.log(cardinality) for cardinality in self.attribute_cardinalities]\n",
        "        log_cardinalities_tensor = tf.constant(log_cardinalities, dtype=tf.float32)\n",
        "        self.log_cardinalities_expanded = tf.expand_dims(log_cardinalities_tensor, axis=-1)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Your custom loss logic here\n",
        "        y_true_splits = tf.split(y_true, self.attribute_cardinalities, axis=1)\n",
        "        y_pred_splits = tf.split(y_pred, self.attribute_cardinalities, axis=1)\n",
        "\n",
        "        max_size = max(self.attribute_cardinalities)\n",
        "\n",
        "        y_true_splits = [tf.pad(split, [[0, 0], [0, max_size - tf.shape(split)[1]]]) for split in y_true_splits]\n",
        "        y_pred_splits = [tf.pad(split, [[0, 0], [0, max_size - tf.shape(split)[1]]]) for split in y_pred_splits]\n",
        "\n",
        "        xent_losses = tf.keras.losses.categorical_crossentropy(y_true_splits, y_pred_splits)\n",
        "\n",
        "        normalized_xent_losses = xent_losses / self.log_cardinalities_expanded\n",
        "\n",
        "        return tf.reduce_mean(normalized_xent_losses, axis=0)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'attribute_cardinalities': self.attribute_cardinalities}\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class AutoencoderModel:\n",
        "    def __init__(self, attribute_cardinalities):\n",
        "        self.INPUT_SHAPE = None\n",
        "        # self.D = 2\n",
        "        self.TEST_SIZE = 0.2\n",
        "        self.MAX_TRIALS = 20\n",
        "        self.EXECUTIONS_PER_TRIAL = 1\n",
        "        self.BATCH_SIZE = 32\n",
        "        self.attribute_cardinalities = attribute_cardinalities\n",
        "\n",
        "        log_cardinalities = [np.log(cardinality) for cardinality in self.attribute_cardinalities]\n",
        "        log_cardinalities_tensor = tf.constant(log_cardinalities, dtype=tf.float32)\n",
        "        self.log_cardinalities_expanded = tf.expand_dims(log_cardinalities_tensor, axis=-1)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'attribute_cardinalities': self.attribute_cardinalities}\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "    def split_train_test(self, df):\n",
        "        # df = df.fillna(0.0)\n",
        "        X_train, X_test = train_test_split(df.copy(), test_size=self.TEST_SIZE)\n",
        "        self.INPUT_SHAPE = X_train.shape[1:]\n",
        "        return X_train.dropna(), X_test.dropna()\n",
        "\n",
        "    def build_encoder(self, hp):\n",
        "        inputs = Input(shape=self.INPUT_SHAPE)\n",
        "        x = Dense(units=hp.Int('encoder_units_1', min_value=160, max_value=160, step=16),\n",
        "              activation='relu',\n",
        "              kernel_regularizer=l2(hp.Choice('encoder_l2_1', [0.0, 0.001])))(inputs)\n",
        "        x = Dropout(hp.Float('encoder_dropout_1', 0.1, 0.1, step=0.1))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(units=hp.Int('encoder_units_2', min_value=16, max_value=16, step=16),\n",
        "              activation='relu',\n",
        "              kernel_regularizer=l2(hp.Choice('encoder_l2_2', [0.0, 0.001])))(x)\n",
        "        x = Dropout(hp.Float('encoder_dropout_2', 0, 0.5, step=0.1))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        latent_dim = hp.Int('latent_space_dim', min_value=2, max_value=50, step=1)\n",
        "        mean = Dense(latent_dim, name='z_mean')(x)\n",
        "        log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "        def sampling(args):\n",
        "            mean, log_var = args\n",
        "            epsilon = tfp.distributions.Normal(0, 1).sample(tf.shape(mean))\n",
        "            return mean + tf.exp(0.5 * log_var) * epsilon\n",
        "\n",
        "        z = Lambda(sampling, output_shape=(latent_dim,), name='z')([mean, log_var])\n",
        "\n",
        "        return Model(inputs, [mean, log_var, z])\n",
        "\n",
        "    def build_decoder(self, hp):\n",
        "        latent_dim = hp.Int('latent_space_dim', min_value=2, max_value=50, step=1)\n",
        "        decoder_inputs = Input(shape=(latent_dim,))\n",
        "\n",
        "        x = Dense(units=hp.Int('decoder_units_1', min_value=16, max_value=256, step=16),\n",
        "              activation='relu',\n",
        "              kernel_regularizer=l2(hp.Choice('decoder_l2_1', [0.0, 0.001, 0.01])))(decoder_inputs)\n",
        "        x = Dropout(hp.Float('decoder_dropout_1', 0.0, 0.0, step=0.1))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(units=hp.Int('decoder_units_2', min_value=160, max_value=256, step=16),\n",
        "              activation='relu',\n",
        "              kernel_regularizer=l2(hp.Choice('decoder_l2_2', [0.0, 0.001])))(x)\n",
        "        x = Dropout(hp.Float('decoder_dropout_2', 0, 0.5, step=0.1))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        decoded_attrs = []\n",
        "        for categories in self.attribute_cardinalities:\n",
        "            decoder_softmax = Dense(categories, activation='softmax')(x)\n",
        "            decoded_attrs.append(decoder_softmax)\n",
        "\n",
        "        outputs = Concatenate()(decoded_attrs)\n",
        "\n",
        "        return Model(decoder_inputs, outputs)\n",
        "\n",
        "    def vae_loss(self, y_true, y_pred, mean, log_var):\n",
        "        reconstruction_loss = CustomCategoricalCrossentropy(attribute_cardinalities=self.attribute_cardinalities)(y_true, y_pred)\n",
        "        kl_loss = -0.5 * tf.reduce_mean(1 + log_var - tf.square(mean) - tf.exp(log_var))\n",
        "        return reconstruction_loss + kl_loss\n",
        "\n",
        "\n",
        "    def build_autoencoder(self, hp):\n",
        "        learning_rate = hp.Choice('learning_rate', values=[1e-3])\n",
        "\n",
        "        autoencoder_input = Input(shape=self.INPUT_SHAPE)\n",
        "        mean, log_var, z = self.build_encoder(hp)(autoencoder_input)\n",
        "        decoder_output = self.build_decoder(hp)(z)\n",
        "        autoencoder = Model(autoencoder_input, decoder_output)\n",
        "\n",
        "        autoencoder.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "            loss=self.vae_loss)\n",
        "\n",
        "        return autoencoder\n",
        "\n",
        "    def define_tuner(self, seed_hps=None):\n",
        "        tuner = BayesianOptimization(\n",
        "            self.build_autoencoder,\n",
        "            objective='val_loss',\n",
        "            max_trials=self.MAX_TRIALS,\n",
        "            executions_per_trial=self.EXECUTIONS_PER_TRIAL,\n",
        "            hyperparameters=seed_hps\n",
        "            )\n",
        "        return tuner"
      ],
      "metadata": {
        "id": "Ag9rlczCY9mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class TestVariationalAutoencoder(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.df = pd.DataFrame({\n",
        "            'col1': ['A', 'B', 'A', 'C', 'B'],\n",
        "            'col2': [1, 2, 1, 2, 2],\n",
        "            'col3': ['X', 'Y', 'X', 'Y', 'Z']\n",
        "        })\n",
        "        self.attribute_cardinalities = [3, 2, 3]\n",
        "\n",
        "        self.variationalautoencoder = VariationalAutoencoder(attribute_cardinalities=self.attribute_cardinalities)\n",
        "\n",
        "    def test_custom_categorical_crossentropy(self):\n",
        "        y_true = np.array([[1, 0, 0, 1, 0, 1, 0, 0],\n",
        "                           [0, 1, 0, 0, 1, 0, 0, 1]])\n",
        "        y_pred = np.array([[0.8, 0.1, 0.1, 0.7, 0.3, 0.9, 0.05, 0.05],\n",
        "                           [0.1, 0.8, 0.1, 0.3, 0.7, 0.05, 0.05, 0.9]])\n",
        "        loss = CustomVAELoss(attribute_cardinalities=self.attribute_cardinalities)(y_true, y_pred)\n",
        "\n",
        "        print(loss)\n",
        "\n",
        "        loss_value = loss.numpy()\n",
        "\n",
        "        self.assertIsInstance(loss_value, float)\n",
        "        self.assertGreaterEqual(loss, 0.0)\n",
        "        self.assertIsNotNone(loss)\n",
        "        #self.assertAlmostEqual(loss.mean(), (0.2231 / np.log(3) + 0.3567 / np.log(2) + 0.1054 / np.log(3)), places=3)\n"
      ],
      "metadata": {
        "id": "w6O9NuxZeTBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "def run_tests(test_class):\n",
        "    suite = unittest.TestLoader().loadTestsFromTestCase(test_class)\n",
        "    runner = unittest.TextTestRunner()\n",
        "    runner.run(suite)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  run_tests(TestVariationalAutoencoder)"
      ],
      "metadata": {
        "id": "hI6pdM-Be9aS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}