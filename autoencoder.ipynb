{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kohathyli/Autoencoders_Census/blob/main/autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "FcGe1770A3ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgC-DNoNxTC1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Dense, BatchNormalization, Concatenate, Dropout, Lambda\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras_tuner.tuners import RandomSearch, BayesianOptimization\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class CustomCategoricalCrossentropy(tf.keras.losses.Loss):\n",
        "    def __init__(self, attribute_cardinalities, name=\"custom_categorical_crossentropy\"):\n",
        "        super(CustomCategoricalCrossentropy, self).__init__(name=name)\n",
        "        self.attribute_cardinalities = attribute_cardinalities\n",
        "        log_cardinalities = [np.log(cardinality) for cardinality in self.attribute_cardinalities]\n",
        "        log_cardinalities_tensor = tf.constant(log_cardinalities, dtype=tf.float32)\n",
        "        self.log_cardinalities_expanded = tf.expand_dims(log_cardinalities_tensor, axis=-1)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Your custom loss logic here\n",
        "        y_true_splits = tf.split(y_true, self.attribute_cardinalities, axis=1)\n",
        "        y_pred_splits = tf.split(y_pred, self.attribute_cardinalities, axis=1)\n",
        "\n",
        "        max_size = max(self.attribute_cardinalities)\n",
        "\n",
        "        y_true_splits = [tf.pad(split, [[0, 0], [0, max_size - tf.shape(split)[1]]]) for split in y_true_splits]\n",
        "        y_pred_splits = [tf.pad(split, [[0, 0], [0, max_size - tf.shape(split)[1]]]) for split in y_pred_splits]\n",
        "\n",
        "        xent_losses = tf.keras.losses.categorical_crossentropy(y_true_splits, y_pred_splits)\n",
        "\n",
        "        normalized_xent_losses = xent_losses / self.log_cardinalities_expanded\n",
        "\n",
        "        return tf.reduce_mean(normalized_xent_losses, axis=0)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'attribute_cardinalities': self.attribute_cardinalities}\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class AutoencoderModel:\n",
        "    def __init__(self, attribute_cardinalities):\n",
        "        self.INPUT_SHAPE = None\n",
        "        # self.D = 2\n",
        "        self.TEST_SIZE = 0.2\n",
        "        self.MAX_TRIALS = 20\n",
        "        self.EXECUTIONS_PER_TRIAL = 1\n",
        "        self.BATCH_SIZE = 32\n",
        "        self.attribute_cardinalities = attribute_cardinalities\n",
        "\n",
        "        log_cardinalities = [np.log(cardinality) for cardinality in self.attribute_cardinalities]\n",
        "        log_cardinalities_tensor = tf.constant(log_cardinalities, dtype=tf.float32)\n",
        "        self.log_cardinalities_expanded = tf.expand_dims(log_cardinalities_tensor, axis=-1)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'attribute_cardinalities': self.attribute_cardinalities}\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "    def split_train_test(self, df):\n",
        "        # df = df.fillna(0.0)\n",
        "        X_train, X_test = train_test_split(df.copy(), test_size=self.TEST_SIZE)\n",
        "        self.INPUT_SHAPE = X_train.shape[1:]\n",
        "        return X_train.dropna(), X_test.dropna()\n",
        "\n",
        "    @staticmethod\n",
        "    def masked_mse(y_true, y_pred):\n",
        "        mask = tf.math.is_finite(y_true)\n",
        "        y_t = tf.where(tf.math.is_finite(y_true), y_true, 0.0)\n",
        "        y_p = tf.where(tf.math.is_finite(y_pred), y_pred, 0.0)\n",
        "        mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
        "        return tf.reduce_mean(mse(y_t*tf.cast(mask, y_t.dtype), y_p*tf.cast(mask, y_p.dtype)))\n",
        "\n",
        "    def build_encoder(self, hp):\n",
        "        inputs = Input(shape=self.INPUT_SHAPE)\n",
        "        x = Dense(units=hp.Int('encoder_units_1', min_value=160, max_value=160, step=16),\n",
        "                  activation='relu',\n",
        "                  kernel_regularizer=l2(hp.Choice('encoder_l2_1', [0.0, 0.001])))(inputs)\n",
        "        x = Dropout(hp.Float('encoder_dropout_1', 0.1, 0.1, step=0.1))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(units=hp.Int('encoder_units_2', min_value=16, max_value=16, step=16),\n",
        "                  activation='relu',\n",
        "                  kernel_regularizer=l2(hp.Choice('encoder_l2_2', [0.0, 0.001])))(x)\n",
        "        x = Dropout(hp.Float('encoder_dropout_2', 0, 0.5, step=0.1))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        latent_space = Dense(units=hp.Int('latent_space_dim', min_value=2, max_value=50, step=1),\n",
        "                             activation='relu')(x)\n",
        "        return Model(inputs, latent_space)\n",
        "\n",
        "    def build_decoder(self, hp):\n",
        "        decoder_inputs = Input(shape=(hp.Int('latent_space_dim', min_value=2, max_value=50, step=1),))\n",
        "        x = Dense(units=hp.Int('decoder_units_1', min_value=16, max_value=256, step=16),\n",
        "                  activation='relu',\n",
        "                  kernel_regularizer=l2(hp.Choice('decoder_l2_1', [0.0, 0.001, 0.01])))(decoder_inputs)\n",
        "        x = Dropout(hp.Float('decoder_dropout_1', 0.0, 0.0, step=0.1))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(units=hp.Int('decoder_units_2', min_value=160, max_value=256, step=16),\n",
        "                  activation='relu',\n",
        "                  kernel_regularizer=l2(hp.Choice('decoder_l2_2', [0.0, 0.001])))(x)\n",
        "        x = Dropout(hp.Float('decoder_dropout_2', 0, 0.5, step=0.1))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        decoded_attrs = []\n",
        "        for categories in self.attribute_cardinalities:\n",
        "          decoder_softmax = Dense(categories, activation='softmax')(x)\n",
        "          decoded_attrs.append(decoder_softmax)\n",
        "\n",
        "        outputs = Concatenate()(decoded_attrs)\n",
        "\n",
        "        return Model(decoder_inputs, outputs)\n",
        "\n",
        "    '''\n",
        "    def custom_categorical_crossentropy(self, y_true, y_pred):\n",
        "        xent_loss = 0\n",
        "        start_idx = 0\n",
        "\n",
        "        for categories in self.attribute_cardinalities:\n",
        "            x_attr = y_true[:, start_idx:start_idx + categories]\n",
        "            y_attr = y_pred[:, start_idx:start_idx + categories]\n",
        "\n",
        "            x_attr = K.cast(x_attr, 'float32')\n",
        "            y_attr = K.cast(y_attr, 'float32')\n",
        "\n",
        "            xent_loss += K.mean(K.categorical_crossentropy(x_attr, y_attr)) / np.log(categories)\n",
        "\n",
        "            start_idx += categories\n",
        "        return xent_loss / len(self.attribute_cardinalities)\n",
        "    '''\n",
        "    '''\n",
        "    def custom_categorical_crossentropy(self, y_true, y_pred):\n",
        "        # We create a separate vector for each attribute\n",
        "        y_true_splits = tf.split(y_true, self.attribute_cardinalities, axis=1)\n",
        "        y_pred_splits = tf.split(y_pred, self.attribute_cardinalities, axis=1)\n",
        "\n",
        "        # Compute the maximum size among the splits\n",
        "        max_size = max(self.attribute_cardinalities)\n",
        "\n",
        "        # Pad each split to have the same size as max_size, this will be useful for calculating\n",
        "        # cross entropy for each vector. If the shape of the vectors is not consistent, the K.categorical_crossentropy\n",
        "        # does not work.\n",
        "        y_true_splits = [tf.pad(split, [[0, 0], [0, max_size - tf.shape(split)[1]]]) for split in y_true_splits]\n",
        "        y_pred_splits = [tf.pad(split, [[0, 0], [0, max_size - tf.shape(split)[1]]]) for split in y_pred_splits]\n",
        "\n",
        "        # Compute the categorical cross-entropy for each attribute\n",
        "        xent_losses = K.categorical_crossentropy(y_true_splits, y_pred_splits)\n",
        "\n",
        "        # Normalize by log of cardinality\n",
        "        normalized_xent_losses = xent_losses / self.log_cardinalities_expanded\n",
        "\n",
        "        return K.mean(normalized_xent_losses,axis=0)\n",
        "    '''\n",
        "\n",
        "    def build_autoencoder(self, hp):\n",
        "        learning_rate = hp.Choice('learning_rate', values=[1e-3])\n",
        "\n",
        "        autoencoder_input = Input(shape=self.INPUT_SHAPE)\n",
        "        encoder_output = self.build_encoder(hp)(autoencoder_input)\n",
        "        decoder_output = self.build_decoder(hp)(encoder_output)\n",
        "        autoencoder = Model(autoencoder_input, decoder_output)\n",
        "        autoencoder.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "            loss=CustomCategoricalCrossentropy(attribute_cardinalities=self.attribute_cardinalities)\n",
        "        )\n",
        "\n",
        "        return autoencoder\n",
        "\n",
        "    def define_tuner(self, seed_hps=None):\n",
        "        tuner = BayesianOptimization(\n",
        "            self.build_autoencoder,\n",
        "            objective='val_loss',\n",
        "            max_trials=self.MAX_TRIALS,\n",
        "            executions_per_trial=self.EXECUTIONS_PER_TRIAL,\n",
        "            hyperparameters=seed_hps\n",
        "            )\n",
        "        return tuner\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class TestAutoencoderModel(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        # Create a DataFrame with synthetic data\n",
        "        self.df = pd.DataFrame({\n",
        "            'col1': ['A', 'B', 'A', 'C', 'B'],\n",
        "            'col2': [1, 2, 1, 2, 2],\n",
        "            'col3': ['X', 'Y', 'X', 'Y', 'Z']\n",
        "        })\n",
        "        self.attribute_cardinalities = [3, 2, 3]  # 3 unique categories in each attribute\n",
        "        self.autoencoder = AutoencoderModel(self.attribute_cardinalities)\n",
        "\n",
        "    def test_split_train_test(self):\n",
        "        X_train, X_test = self.autoencoder.split_train_test(self.df)\n",
        "        self.assertEqual(len(X_train) + len(X_test), len(self.df))\n",
        "\n",
        "    def test_masked_mse(self):\n",
        "        y_true = np.array([[1.0, np.nan], [2.0, 2.0]])\n",
        "        y_pred = np.array([[1.0, 1.0], [3.0, 2.0]])\n",
        "        result = self.autoencoder.masked_mse(y_true, y_pred)\n",
        "        self.assertIsNotNone(result)\n",
        "\n",
        "    def test_custom_categorical_crossentropy(self):\n",
        "        y_true = np.array([[1, 0, 0, 1, 0, 1, 0, 0],\n",
        "                           [0, 1, 0, 0, 1, 0, 0, 1]])\n",
        "        y_pred = np.array([[0.8, 0.1, 0.1, 0.7, 0.3, 0.9, 0.05, 0.05],\n",
        "                           [0.1, 0.8, 0.1, 0.3, 0.7, 0.05, 0.05, 0.9]])\n",
        "        result = CustomCategoricalCrossentropy(attribute_cardinalities=self.attribute_cardinalities)(y_true, y_pred)\n",
        "        # p=[1,0,0],q=[0.8,0.1,0.1]: 0.2231 / np.log(3)\n",
        "        # p=[1,0],q=[0.7,0.3]: 0.3567 / np.log(2)\n",
        "        # p=[1,0,0],q=[0.9,0.05,0.05]: 0.1054 / np.log(3)\n",
        "        # p=[0,1,0],q=[0.1,0.8,0.1]: 0.2231 / np.log(3)\n",
        "        # p=[0,1,0],q=[0.3,0.7]: 0.3567 / np.log(2)\n",
        "        # p=[0,0,1],q=[0.05,0.05,0.9]: 0.1054 / np.log(3)\n",
        "        print(result)\n",
        "        self.assertIsNotNone(result)\n",
        "        # We divide by 2 because we have two examples\n",
        "        # We divide by 3 because we have 3 attributes per example\n",
        "        # We divide by np.log(3) for normalization (all attributes have cardinality 3)\n",
        "        self.assertAlmostEqual(result.mean(), (0.2231/ np.log(3) + 0.3567/ np.log(2) + 0.1054/ np.log(3))/3, places=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "OcWSTfn9vVPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "def run_tests(test_class):\n",
        "    suite = unittest.TestLoader().loadTestsFromTestCase(test_class)\n",
        "    runner = unittest.TextTestRunner()\n",
        "    runner.run(suite)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  run_tests(TestAutoencoderModel)"
      ],
      "metadata": {
        "id": "sURHG2b_wOjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kerastuner import HyperModel\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class CustomCategoricalCrossentropy(tf.keras.losses.Loss):\n",
        "    def __init__(self, attribute_cardinalities, name=\"custom_categorical_crossentropy\"):\n",
        "        super(CustomCategoricalCrossentropy, self).__init__(name=name)\n",
        "        self.attribute_cardinalities = attribute_cardinalities\n",
        "        log_cardinalities = [np.log(cardinality) for cardinality in self.attribute_cardinalities]\n",
        "        log_cardinalities_tensor = tf.constant(log_cardinalities, dtype=tf.float32)\n",
        "        self.log_cardinalities_expanded = tf.expand_dims(log_cardinalities_tensor, axis=-1)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_true_splits = tf.split(y_true, self.attribute_cardinalities, axis=1)\n",
        "        y_pred_splits = tf.split(y_pred, self.attribute_cardinalities, axis=1)\n",
        "\n",
        "        max_size = max(self.attribute_cardinalities)\n",
        "\n",
        "        y_true_splits = [tf.pad(split, [[0, 0], [0, max_size - tf.shape(split)[1]]]) for split in y_true_splits]\n",
        "        y_pred_splits = [tf.pad(split, [[0, 0], [0, max_size - tf.shape(split)[1]]]) for split in y_pred_splits]\n",
        "\n",
        "        xent_losses = tf.keras.losses.categorical_crossentropy(y_true_splits, y_pred_splits)\n",
        "\n",
        "        normalized_xent_losses = xent_losses / self.log_cardinalities_expanded\n",
        "\n",
        "        reconstruction_loss = tf.reduce_mean(normalized_xent_losses)\n",
        "\n",
        "        kl_divergence = -0.5 * tf.reduce_sum(1 + y_pred - tf.square(y_pred) - tf.exp(y_pred), axis=-1)\n",
        "        kl_divergence = tf.reduce_mean(kl_divergence)\n",
        "\n",
        "        total_loss = reconstruction_loss + kl_divergence\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'attribute_cardinalities': self.attribute_cardinalities}\n",
        "\n",
        "class VariationalAutoencoderModel:\n",
        "    def __init__(self, attribute_cardinalities):\n",
        "        self.INPUT_SHAPE = None\n",
        "        # self.D = 2\n",
        "        self.TEST_SIZE = 0.2\n",
        "        self.MAX_TRIALS = 20\n",
        "        self.EXECUTIONS_PER_TRIAL = 1\n",
        "        self.BATCH_SIZE = 32\n",
        "        self.attribute_cardinalities = attribute_cardinalities\n",
        "\n",
        "        log_cardinalities = [np.log(cardinality) for cardinality in self.attribute_cardinalities]\n",
        "        log_cardinalities_tensor = tf.constant(log_cardinalities, dtype=tf.float32)\n",
        "        self.log_cardinalities_expanded = tf.expand_dims(log_cardinalities_tensor, axis=-1)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'attribute_cardinalities': self.attribute_cardinalities}\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "    def split_train_test(self, df):\n",
        "        # df = df.fillna(0.0)\n",
        "        X_train, X_test = train_test_split(df.copy(), test_size=self.TEST_SIZE)\n",
        "        self.INPUT_SHAPE = X_train.shape[1:]\n",
        "        return X_train.dropna(), X_test.dropna()\n",
        "\n",
        "    #def split_train_test(self, data, test_size=0.2):\n",
        "        #train_size = int((1 - test_size) * len(data))\n",
        "        #train_data, test_data = data[:train_size], data[train_size:]\n",
        "        #return train_data, test_data\n",
        "\n",
        "    def build_encoder(self, hp):\n",
        "        inputs = Input(shape=self.INPUT_SHAPE)\n",
        "        x = Dense(units=hp.Int('encoder_units_1', min_value=160, max_value=160, step=16),\n",
        "                  activation='relu',\n",
        "                  kernel_regularizer=l2(hp.Choice('encoder_l2_1', [0.0, 0.001])))(inputs)\n",
        "        x = Dropout(hp.Float('encoder_dropout_1', 0.1, 0.1, step=0.1))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(units=hp.Int('encoder_units_2', min_value=16, max_value=16, step=16),\n",
        "                  activation='relu',\n",
        "                  kernel_regularizer=l2(hp.Choice('encoder_l2_2', [0.0, 0.001])))(x)\n",
        "        x = Dropout(hp.Float('encoder_dropout_2', 0, 0.5, step=0.1))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        z_mean = Dense(hp.Int('latent_space_dim', min_value=2, max_value=50, step=1), activation='linear')(x)\n",
        "        z_log_var = Dense(hp.Int('latent_space_dim', min_value=2, max_value=50, step=1), activation='linear')(x)\n",
        "\n",
        "        # Sampling function for the VAE\n",
        "        def sampling(args):\n",
        "            z_mean, z_log_var = args\n",
        "            batch = tf.shape(z_mean)[0]\n",
        "            dim = tf.shape(z_mean)[1]\n",
        "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "        latent_space = tf.keras.layers.Lambda(sampling, output_shape=(hp.Int('latent_space_dim', min_value=2, max_value=50, step=1),))([z_mean, z_log_var])\n",
        "\n",
        "        return Model(inputs, [z_mean, z_log_var, latent_space])\n",
        "\n",
        "    def build_decoder(self, hp):\n",
        "        decoder_inputs = Input(shape=(hp.Int('latent_space_dim', min_value=2, max_value=50, step=1),))\n",
        "        x = Dense(units=hp.Int('decoder_units_1', min_value=16, max_value=256, step=16),\n",
        "                  activation='relu',\n",
        "                  kernel_regularizer=l2(hp.Choice('decoder_l2_1', [0.0, 0.001, 0.01])))(decoder_inputs)\n",
        "        x = Dropout(hp.Float('decoder_dropout_1', 0.0, 0.0, step=0.1))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(units=hp.Int('decoder_units_2', min_value=160, max_value=256, step=16),\n",
        "                  activation='relu',\n",
        "                  kernel_regularizer=l2(hp.Choice('decoder_l2_2', [0.0, 0.001])))(x)\n",
        "        x = Dropout(hp.Float('decoder_dropout_2', 0, 0.5, step=0.1))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        decoded_attrs = []\n",
        "        for categories in self.attribute_cardinalities:\n",
        "          decoder_softmax = Dense(categories, activation='softmax')(x)\n",
        "          decoded_attrs.append(decoder_softmax)\n",
        "\n",
        "        outputs = Concatenate()(decoded_attrs)\n",
        "\n",
        "        return Model(decoder_inputs, outputs)\n",
        "\n",
        "    def build_autoencoder(self, hp):\n",
        "\n",
        "        learning_rate = hp.Choice('learning_rate', values=[1e-3])\n",
        "\n",
        "        autoencoder_input = Input(shape=self.INPUT_SHAPE)\n",
        "\n",
        "        encoder_output = self.build_encoder(hp)(autoencoder_input)\n",
        "        z_mean, z_log_var, latent_space = encoder_output\n",
        "\n",
        "        def sampling(args):\n",
        "            z_mean, z_log_var = args\n",
        "            batch = tf.shape(z_mean)[0]\n",
        "            dim = tf.shape(z_mean)[1]\n",
        "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "        latent_sample = tf.keras.layers.Lambda(sampling, output_shape=(hp.Int('latent_space_dim', min_value=2, max_value=50, step=1),))([z_mean, z_log_var])\n",
        "\n",
        "        decoder_output = self.build_decoder(hp)(latent_sample)\n",
        "\n",
        "        autoencoder = Model(autoencoder_input, decoder_output)\n",
        "        autoencoder.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "            loss=CustomCategoricalCrossentropy(attribute_cardinalities=self.attribute_cardinalities)\n",
        "        )\n",
        "        return autoencoder\n",
        "\n",
        "    def define_tuner(self, seed_hps=None):\n",
        "        tuner = BayesianOptimization(\n",
        "            self.build_autoencoder,\n",
        "            objective='val_loss',\n",
        "            max_trials=self.MAX_TRIALS,\n",
        "            executions_per_trial=self.EXECUTIONS_PER_TRIAL,\n",
        "            hyperparameters=seed_hps\n",
        "            )\n",
        "        return tuner"
      ],
      "metadata": {
        "id": "hz_1k71-7pY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestVariationalAutoencoderModel(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.df = pd.DataFrame({\n",
        "            'col1': ['A', 'B', 'A', 'C', 'B'],\n",
        "            'col2': [1, 2, 1, 2, 2],\n",
        "            'col3': ['X', 'Y', 'X', 'Y', 'Z']\n",
        "        })\n",
        "        self.attribute_cardinalities = [3, 2, 3]\n",
        "        self.vae_model = VariationalAutoencoderModel(self.attribute_cardinalities)\n",
        "\n",
        "    def test_split_train_test(self):\n",
        "        X_train, X_test = self.vae_model.split_train_test(self.df)\n",
        "        self.assertEqual(len(X_train) + len(X_test), len(self.df))\n",
        "\n",
        "    def test_custom_categorical_crossentropy(self):\n",
        "        y_true = np.array([[1, 0, 0, 1, 0, 1, 0, 0],\n",
        "                           [0, 1, 0, 0, 1, 0, 0, 1]])\n",
        "        y_pred = np.array([[0.8, 0.1, 0.1, 0.7, 0.3, 0.9, 0.05, 0.05],\n",
        "                           [0.1, 0.8, 0.1, 0.3, 0.7, 0.05, 0.05, 0.9]])\n",
        "        result = CustomCategoricalCrossentropy(attribute_cardinalities=self.attribute_cardinalities)(y_true, y_pred)\n",
        "        print(result)\n",
        "        self.assertIsNotNone(result)\n",
        "        epsilon = 1e-10\n",
        "\n",
        "        kl_divergence = np.mean(np.sum(\n",
        "            y_true * np.log((y_true + epsilon) / (y_pred + epsilon)) +\n",
        "            (1 - y_true) * np.log((1 - y_true + epsilon) / (1 - y_pred + epsilon)),\n",
        "            axis=1\n",
        "        ))\n",
        "        print(kl_divergence)\n",
        "        self.assertAlmostEqual(result.mean(), np.sum((0.2231/ np.log(3) + 0.3567/ np.log(2) + 0.1054/ np.log(3))/3+ kl_divergence), places=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "QYQWFyFA9Qpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "def run_tests(test_class):\n",
        "    suite = unittest.TestLoader().loadTestsFromTestCase(test_class)\n",
        "    runner = unittest.TextTestRunner()\n",
        "    runner.run(suite)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  run_tests(TestVariationalAutoencoderModel)"
      ],
      "metadata": {
        "id": "2IQZAetk_hqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BWaaCYrGmMsI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}